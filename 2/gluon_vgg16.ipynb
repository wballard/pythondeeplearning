{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our final model, VGG16 in Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 10 # 10 output digits\n",
    "batch_size = 128 # mini batch\n",
    "epochs = 10 # total training loops\n",
    "learning_rate = 0.01 # amount we update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR images again. Note the need to transform so we are (color channel, x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32) / 255.0\n",
    "    return data, label\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.CIFAR10(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.CIFAR10(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the VGG blocks, 5 blocks in all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [3, 3, 3, 3, 3]\n",
    "filters = [64, 128, 256, 512, 512]\n",
    "repeats = [2, 2, 3, 3, 3]\n",
    "pooling = [2, 2, 2, 2, 2]\n",
    "strides = [2, 2, 2, 2, 2]\n",
    "dense_units = [4096, 4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again --  you don't have a final softmax *layer*, MxNet handles he softmax inside this loss function, so the output is a straight linear mapping -- no activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = gluon.nn.HybridSequential()\n",
    "with vgg16.name_scope():\n",
    "    for kernel, filter, pool, stride, repeat in zip(kernels, filters, pooling, strides, repeats):\n",
    "        for _ in range(0, repeat):\n",
    "            vgg16.add(gluon.nn.Conv2D(channels=filter, \n",
    "                                    kernel_size=kernel,\n",
    "                                    padding=(kernel//2),\n",
    "                                    activation='relu'))\n",
    "        vgg16.add(gluon.nn.MaxPool2D(pool_size=pool, \n",
    "                                       strides=stride,\n",
    "                                       padding=(kernel//2)))\n",
    "\n",
    "    vgg16.add(gluon.nn.Flatten())\n",
    "\n",
    "    for units in dense_units:\n",
    "        vgg16.add(gluon.nn.Dense(units, activation='relu'))\n",
    "    \n",
    "    vgg16.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon requires a context, either `cpu` or `gpu`. You can change this to `cpu` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.collect_params().initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let't take a look at the resulting network. We need to feed in a sample batch to infer the network size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "        Layer (type)                                Output Shape         Param #\n",
      "================================================================================\n",
      "               Input                            (128, 3, 32, 32)               0\n",
      "        Activation-1   <Symbol hybridsequential0_conv0_relu_fwd>               0\n",
      "        Activation-2                           (128, 64, 32, 32)               0\n",
      "            Conv2D-3                           (128, 64, 32, 32)            1792\n",
      "        Activation-4   <Symbol hybridsequential0_conv1_relu_fwd>               0\n",
      "        Activation-5                           (128, 64, 32, 32)               0\n",
      "            Conv2D-6                           (128, 64, 32, 32)           36928\n",
      "         MaxPool2D-7                           (128, 64, 17, 17)               0\n",
      "        Activation-8   <Symbol hybridsequential0_conv2_relu_fwd>               0\n",
      "        Activation-9                          (128, 128, 17, 17)               0\n",
      "           Conv2D-10                          (128, 128, 17, 17)           73856\n",
      "       Activation-11   <Symbol hybridsequential0_conv3_relu_fwd>               0\n",
      "       Activation-12                          (128, 128, 17, 17)               0\n",
      "           Conv2D-13                          (128, 128, 17, 17)          147584\n",
      "        MaxPool2D-14                            (128, 128, 9, 9)               0\n",
      "       Activation-15   <Symbol hybridsequential0_conv4_relu_fwd>               0\n",
      "       Activation-16                            (128, 256, 9, 9)               0\n",
      "           Conv2D-17                            (128, 256, 9, 9)          295168\n",
      "       Activation-18   <Symbol hybridsequential0_conv5_relu_fwd>               0\n",
      "       Activation-19                            (128, 256, 9, 9)               0\n",
      "           Conv2D-20                            (128, 256, 9, 9)          590080\n",
      "       Activation-21   <Symbol hybridsequential0_conv6_relu_fwd>               0\n",
      "       Activation-22                            (128, 256, 9, 9)               0\n",
      "           Conv2D-23                            (128, 256, 9, 9)          590080\n",
      "        MaxPool2D-24                            (128, 256, 5, 5)               0\n",
      "       Activation-25   <Symbol hybridsequential0_conv7_relu_fwd>               0\n",
      "       Activation-26                            (128, 512, 5, 5)               0\n",
      "           Conv2D-27                            (128, 512, 5, 5)         1180160\n",
      "       Activation-28   <Symbol hybridsequential0_conv8_relu_fwd>               0\n",
      "       Activation-29                            (128, 512, 5, 5)               0\n",
      "           Conv2D-30                            (128, 512, 5, 5)         2359808\n",
      "       Activation-31   <Symbol hybridsequential0_conv9_relu_fwd>               0\n",
      "       Activation-32                            (128, 512, 5, 5)               0\n",
      "           Conv2D-33                            (128, 512, 5, 5)         2359808\n",
      "        MaxPool2D-34                            (128, 512, 3, 3)               0\n",
      "       Activation-35  <Symbol hybridsequential0_conv10_relu_fwd>               0\n",
      "       Activation-36                            (128, 512, 3, 3)               0\n",
      "           Conv2D-37                            (128, 512, 3, 3)         2359808\n",
      "       Activation-38  <Symbol hybridsequential0_conv11_relu_fwd>               0\n",
      "       Activation-39                            (128, 512, 3, 3)               0\n",
      "           Conv2D-40                            (128, 512, 3, 3)         2359808\n",
      "       Activation-41  <Symbol hybridsequential0_conv12_relu_fwd>               0\n",
      "       Activation-42                            (128, 512, 3, 3)               0\n",
      "           Conv2D-43                            (128, 512, 3, 3)         2359808\n",
      "        MaxPool2D-44                            (128, 512, 2, 2)               0\n",
      "          Flatten-45                                 (128, 2048)               0\n",
      "       Activation-46  <Symbol hybridsequential0_dense0_relu_fwd>               0\n",
      "       Activation-47                                 (128, 4096)               0\n",
      "            Dense-48                                 (128, 4096)         8392704\n",
      "       Activation-49  <Symbol hybridsequential0_dense1_relu_fwd>               0\n",
      "       Activation-50                                 (128, 4096)               0\n",
      "            Dense-51                                 (128, 4096)        16781312\n",
      "            Dense-52                                   (128, 10)           40970\n",
      "================================================================================\n",
      "Parameters in forward computation graph, duplicate included\n",
      "   Total params: 39929674\n",
      "   Trainable params: 39929674\n",
      "   Non-trainable params: 0\n",
      "Shared params in forward computation graph: 0\n",
      "Unique parameters in model: 39929674\n",
      "--------------------------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i, (d, l) in enumerate(train_data):\n",
    "    print(vgg16.summary(d.as_in_context(ctx)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you network doesn't change shape, you can `hybridize` it, which makes Gluon run in a precomplied mode much like Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as always, learning is done with an optimizer and a loss function, learning a classifier with categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(vgg16.collect_params(), 'sgd', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.3023775815029266, Train_acc 0.13882, Test_acc 0.1424, Time 25.382986\n",
      "Epoch 1. Loss: 2.302042154423849, Train_acc 0.12938, Test_acc 0.1307, Time 24.062614\n",
      "Epoch 2. Loss: 2.3013975877078985, Train_acc 0.18008, Test_acc 0.1877, Time 24.478345\n",
      "Epoch 3. Loss: 2.2999088895211575, Train_acc 0.15604, Test_acc 0.1578, Time 24.127373\n",
      "Epoch 4. Loss: 2.295473200023417, Train_acc 0.17238, Test_acc 0.1762, Time 24.556334\n",
      "Epoch 5. Loss: 2.2741609553267152, Train_acc 0.18428, Test_acc 0.1857, Time 24.191865\n",
      "Epoch 6. Loss: 2.1467806422007767, Train_acc 0.21324, Test_acc 0.218, Time 24.074978\n",
      "Epoch 7. Loss: 2.0475947909910936, Train_acc 0.23612, Test_acc 0.2464, Time 24.471702\n",
      "Epoch 8. Loss: 1.9994886837158794, Train_acc 0.28078, Test_acc 0.2859, Time 24.481846\n",
      "Epoch 9. Loss: 1.9556362338472784, Train_acc 0.2652, Test_acc 0.2716, Time 24.459939\n"
     ]
    }
   ],
   "source": [
    "smoothing_constant = .01\n",
    "moving_loss = 0.0\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time()\n",
    "    for i, (d, l) in enumerate(train_data):\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = vgg16(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        #  Keep a moving average of the losses\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "    elapsed = time() - start\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, vgg16)\n",
    "    train_accuracy = evaluate_accuracy(train_data, vgg16)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s, Time %f\" % (e, moving_loss, train_accuracy, test_accuracy, elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
