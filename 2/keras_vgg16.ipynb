{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an VGG16 in Keras. This is quite the more compliated network, in that it goes even deeper, running convolutions over convolutions.\n",
    "\n",
    "Keras actually has a related repository https://github.com/keras-team/keras-applications/ where you can find a built in, and pretrained VGG16. We'll be implementing it ourselves to get more a sense of how it goes together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the CIFAR images, normalize the images on all color channels 0-1, and one hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 10 # 10 output digits\n",
    "batch_size = 128 # mini batch\n",
    "epochs = 10 # total training loops\n",
    "learning_rate = 0.01 # amount we update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = np.expand_dims(train_images / np.max(train_images), -1)\n",
    "test_images = np.expand_dims(test_images / np.max(test_images), -1)\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_outputs)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 has 5 blocks, each stacking convolutions. These double in filtering as they grow, which was inspired from older machine vision techniques of 'pyramids'. Think of the layers as forming a kind of step pyramid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_kernels = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
    "convolution_filters = [64, 128, 256, 512, 512]\n",
    "convolution_repeats = [2, 2, 3, 3, 3]\n",
    "convolutional_pooling = (2, 2)\n",
    "dense_units = [4096, 4096]\n",
    "image_shape = train_images.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a loop in a loop, adding convolutional layers end to end before taking a max pooling, looking for the strongest signals. This kind of pyramidal attenuation of filters is a relatively popular approach, and you will see this pattern appear in many different network architectures.\n",
    "\n",
    "We'll put in one placeholder layer to contain the image shape extracted frome the training data.\n",
    "\n",
    "Note the use of `same` padding. This actually will pad the images. We need to do this here so that the input image is in fact big enough to 'divide' this many times. You'll see we in the final convolution we end up with a very small x and y dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1, 4096)        2101248   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,638,218\n",
      "Trainable params: 33,638,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = Sequential()\n",
    "vgg16.add(Reshape(image_shape[:-1], input_shape=image_shape))\n",
    "for kernel, filters, repeats in zip(convolution_kernels, convolution_filters, convolution_repeats):\n",
    "    for _ in range(0, repeats):\n",
    "        vgg16.add(Conv2D(filters, kernel, activation='relu', padding='same'))\n",
    "    vgg16.add(MaxPooling2D(convolutional_pooling, padding='same'))\n",
    "for units in dense_units:\n",
    "    vgg16.add(Dense(units, activation='relu'))\n",
    "vgg16.add(Flatten())\n",
    "vgg16.add(Dense(num_outputs, activation='softmax'))\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as always, learning is done with an optimizer and a loss function, learning a classifier with categorical cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "loss = keras.losses.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, keep in mind this is starting to be a pretty big model. If you train this on a CPU, it is *possible*, but it is going to take a very long time. I'm running on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 22s 433us/step - loss: 2.3025 - acc: 0.1019 - val_loss: 2.3024 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 2.3024 - acc: 0.1060 - val_loss: 2.3023 - val_acc: 0.1221\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 2.3023 - acc: 0.1075 - val_loss: 2.3022 - val_acc: 0.1041\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 2.3022 - acc: 0.1113 - val_loss: 2.3021 - val_acc: 0.1099\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 2.3020 - acc: 0.1229 - val_loss: 2.3018 - val_acc: 0.1618\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 2.3017 - acc: 0.1327 - val_loss: 2.3014 - val_acc: 0.1481\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 2.3011 - acc: 0.1413 - val_loss: 2.3006 - val_acc: 0.1624\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 2.2998 - acc: 0.1602 - val_loss: 2.2986 - val_acc: 0.1623\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 2.2963 - acc: 0.1768 - val_loss: 2.2925 - val_acc: 0.1882\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 2.2832 - acc: 0.1863 - val_loss: 2.2648 - val_acc: 0.1865\n"
     ]
    }
   ],
   "source": [
    "vgg16.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = vgg16.fit(train_images, train_labels,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is a less accurate model than AlexNet for 10 epochs, even though it has less trainable parameters. The network architecture matters, and not all parameters are created equal!\n",
    "\n",
    "However -- notice that the model is not overfit -- acc and val_acc are very close. We can simply keep training for more epochs until we flatline -- no longer improving the loss and accuracy. And will likely need to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
