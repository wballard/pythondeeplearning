{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the good old MNIST digits, we'll explore hyper parameters. First up -- let's load and normalize the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, MaxPooling2D, Conv2D, BatchNormalization, ZeroPadding2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "import keras.wrappers.scikit_learn\n",
    "import scipy.stats\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.expand_dims(x_train / np.max(x_train), -1)\n",
    "test_images = np.expand_dims(x_test / np.max(x_test), -1)\n",
    "train_labels = keras.utils.to_categorical(y_train, 10)\n",
    "test_labels = keras.utils.to_categorical(y_test, 10)\n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same build function we used with Grid Search, but just as a reminder -- the idea is to adapt Keras interface to Scikit learn interface.\n",
    "\n",
    "Scikit learn has a great 'standardized' interface that we can use with keras. This requires a 'build function' -- which creates a model.\n",
    "\n",
    "This build function has a set of parameters -- hyperparameters -- that we will vary in order to find a 'best' model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_images[0].shape\n",
    "num_classes = 10\n",
    "\n",
    "def builder(activation='relu', depth=1, pool=3, \n",
    "            strides=2, filters=32, hidden=64, dropout=0.0,\n",
    "            loss='categorical_crossentropy', optimzer='adam'):\n",
    "    model = Sequential()\n",
    "    # initial reshape to have consistent layering\n",
    "    model.add(Reshape(input_shape, input_shape=input_shape))\n",
    "    \n",
    "    # convolutional stack\n",
    "    for i in range(depth):\n",
    "        model.add(Conv2D(filters, pool, activation=activation))\n",
    "        model.add(ZeroPadding2D(pool//2))\n",
    "        model.add(MaxPooling2D(pool, strides=strides))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    \n",
    "    # multilayer perceptron\n",
    "    model.add(Flatten())\n",
    "    for i in range(depth):\n",
    "        model.add(Dense(hidden, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    # final class activation\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimzer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "classifier = keras.wrappers.scikit_learn.KerasClassifier(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.1622 - acc: 0.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac56f87f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have separated testing data, we'll use a custom cross validator to split at the index point where the training images end, and then just concatenate the training and testing data end to end. \n",
    "\n",
    "The `PredefinedSplit` let's us declare which samples are training `-1` and testing folds. We'll use only one testing fold, with a value of `0`, which is the 0 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e90065b6056e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m train_test_bitmap = np.concatenate(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     axis=0)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "all_images = np.concatenate((train_images, test_images), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "train_test_bitmap = np.concatenate(\n",
    "    (np.full(len(train_images), -1), np.zeros(len(test_images))),\n",
    "    axis=0)\n",
    "\n",
    "train_test_data = PredefinedSplit(train_test_bitmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the actual randomized search. This is different than grid search in that each iteration, which is a kind of meta-epoch, takes a random sample for each parameter. This sampling is done on a probability distribution, so you need to think about what kind of numbers you need `int` or `float`, as well as the range of values that you think are valid.\n",
    "\n",
    "And -- you need to pick a probability distribution, whether something flat, a uniform distribution like how dice or a quarter work, or something more sophisticated like a gaussian or an exponential that samples values with different probabilities.\n",
    "\n",
    "There are no hard and fast rules to the best distribution to use, they are arrived at by experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential\n",
    "Here is a look at what the values are like in an exponential, first with the default scale of 1, and then with a scale of 0.1.\n",
    "\n",
    "Scikit learn is taking a large number of random samples, so we'll simulate that by making a list of sampled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'scatterplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-955190afdb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexponential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexponential\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'scatterplot'"
     ]
    }
   ],
   "source": [
    "sample_indexes = [i for i in range(0, 100)]\n",
    "exponential = scipy.stats.expon()\n",
    "samples = [exponential.rvs() for x in sample_indexes]\n",
    "sns.scatterplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform\n",
    "This is really two options -- uniform float and uniform integers, let's take a look at both. You'll see the numbers are evenly spread across the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'scatterplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-af1516f0f73b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'scatterplot'"
     ]
    }
   ],
   "source": [
    "sample_indexes = [i for i in range(0, 100)]\n",
    "uniform = scipy.stats.uniform(1, 10)\n",
    "samples = [uniform.rvs() for x in sample_indexes]\n",
    "sns.scatterplot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'scatterplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2060dd7f46ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'scatterplot'"
     ]
    }
   ],
   "source": [
    "sample_indexes = [i for i in range(0, 100)]\n",
    "integers = scipy.stats.randint(32, 64)\n",
    "samples = [integers.rvs() for x in sample_indexes]\n",
    "sns.scatterplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal\n",
    "Use this when you want numbers to hover around a midpoint, by default around zero. Now notice in this graph that this probably doesn't look like the bell curve you would expect -- that's because we have the sample index as the X axis. What you can see is the density is around zero -- that's the peak of the bell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'scatterplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3fed51d51103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'scatterplot'"
     ]
    }
   ],
   "source": [
    "sample_indexes = [i for i in range(0, 100)]\n",
    "normal = scipy.stats.norm(loc=0)\n",
    "samples = [normal.rvs() for x in sample_indexes]\n",
    "sns.scatterplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized search doesn't cover every possible permutation of parameters, it runs for `n_iter` iterations. Just a warning, this is going to take a **long while to run**. However, you can control the number of iterations to suit your time, patience, or compute budget.\n",
    "\n",
    "The second thing to note is you can mix lists of parameters with distributions. The lists get a random pick from the options. The distributions get a random value generated. This is pretty cool, you can mix nominal parameters like activations with numerical parameters easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 64 candidates, totalling 64 fits\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.1551 - acc: 0.9535\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.0558 - acc: 0.9829\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0391 - acc: 0.9881\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0298 - acc: 0.9905\n",
      "10000/10000 [==============================] - 1s 94us/step\n",
      "60000/60000 [==============================] - 5s 82us/step\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.1425 - acc: 0.9579\n",
      "Epoch 2/4\n",
      "47264/60000 [======================>.......] - ETA: 2s - loss: 0.0514 - acc: 0.9844"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'epochs': [4, 8],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'depth': [1, 2],\n",
    "    'filters': [32, 64],\n",
    "    'hidden': [64, 128],\n",
    "    'dropout': [0.0, 0.25]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=classifier, \n",
    "                    param_grid=param_grid, \n",
    "                    refit=False,\n",
    "                    verbose=1,\n",
    "                    n_iter=16,\n",
    "                    cv=train_test_data)\n",
    "grid_result = grid.fit(all_images, all_labels)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is often a lot of human waiting, but it beats sitting and manually supervising experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
